# -*- coding: utf-8 -*-
"""1210_steering_OLMo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OQthi9Xzk7Gl_1lxX6IbnB6HRwZ62jbL
"""

!pip install -q transformers accelerate sentencepiece torch datasets scikit-learn scipy seaborn

from huggingface_hub import login
login()

# ------------------------------------------------------------
# 0. Imports & Model Load
# ------------------------------------------------------------

import os
import re
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import spearmanr
import seaborn as sns
from tqdm.auto import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

model_name = "allenai/Olmo-3-1025-7B"
print("Loading:", model_name)

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)

model.eval()

print("Num layers:", model.config.num_hidden_layers)
print("Hidden dim:", model.config.hidden_size)

# ------------------------------------------------------------
# 1. Utility Functions (Embedding Extraction)
# ------------------------------------------------------------

def extract_hidden_single(text: str, layer: int):
    """
    Returns mean-pooled hidden embedding at a specific layer.
    """
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=200
    ).to(model.device)

    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        h = outputs.hidden_states[layer]          # [1, seq, dim]
        mask = inputs["attention_mask"].unsqueeze(-1)

        masked = h * mask
        summed = masked.sum(dim=1)
        length = mask.sum(dim=1).clamp(min=1)
        emb = summed / length                     # [1, dim]

    return emb.squeeze(0)   # ‚úÖ GPU tensor Í∑∏ÎåÄÎ°ú Î∞òÌôò

# ------------------------------------------------------------
# 2. Cosine Similarity
# ------------------------------------------------------------

def cos_sim(a, b):
    a = a / (a.norm(p=2) + 1e-8)
    b = b / (b.norm(p=2) + 1e-8)
    return float((a * b).sum())

# ------------------------------------------------------------
# 3. Layer Detection (Internal Embedding Difference)
# ------------------------------------------------------------

def extract_last_token_states(df, model, tokenizer):
    cache = {}

    for _, row in df.iterrows():
        for key in ["logical", "pragmatic"]:
            sent = row[key]

            if sent not in cache:
                inputs = tokenizer(
                    sent,
                    return_tensors="pt",
                    truncation=True,
                    max_length=128
                ).to(model.device)

                with torch.no_grad():
                    outputs = model(
                        **inputs,
                        output_hidden_states=True
                    )

                hidden_states = outputs.hidden_states

                layer_embs = []
                for layer in range(len(hidden_states)):
                    emb = hidden_states[layer][0, -1].detach().cpu()
                    layer_embs.append(emb)

                cache[sent] = layer_embs

    return cache


def cosine_score_for_layer_cached(df, cache, layer):
    scores = []

    for _, row in df.iterrows():
        l = cache[row["logical"]][layer].numpy().reshape(1, -1)
        p = cache[row["pragmatic"]][layer].numpy().reshape(1, -1)

        sim_lp = cosine_similarity(l, p)[0][0]
        scores.append(1 - sim_lp)  # separation score

    return np.mean(scores)


def detect_layer(df, model, tokenizer):
    print("\nDetecting layers ...")

    cache = extract_last_token_states(df, model, tokenizer)

    for L in range(model.config.num_hidden_layers):
        s = cosine_score_for_layer_cached(df, cache, L)
        print(f"Layer {L}: score={s:.4f}")

# ------------------------------------------------------------
# 4. Steering Vector
# ------------------------------------------------------------

def compute_steering_vector(df, layer):
    logical_vecs = []
    pragmatic_vecs = []

    for _, row in df.iterrows():
        logical_vecs.append(extract_hidden_single(row["logical"], layer))
        pragmatic_vecs.append(extract_hidden_single(row["pragmatic"], layer))

    logical_mean = torch.stack(logical_vecs).mean(dim=0)
    pragmatic_mean = torch.stack(pragmatic_vecs).mean(dim=0)

    v = pragmatic_mean - logical_mean
    v = v / v.norm()     # normalize
    return v

# ------------------------------------------------------------
# 5. Compute steering direction from multiple layers
# ------------------------------------------------------------


def compute_direction_multi_layers(df, layers):
    vecs = []

    print(f"\nComputing steering direction from {len(df)} samples...")

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Computing direction"):
        p = row["pragmatic"]
        l = row["logical"]

        vec_p = []
        vec_l = []

        for layer in layers:
            vec_p.append(extract_hidden_single(p, layer))
            vec_l.append(extract_hidden_single(l, layer))

        vec_p = torch.cat(vec_p)
        vec_l = torch.cat(vec_l)

        vecs.append(vec_p - vec_l)

    direction = torch.stack(vecs).mean(dim=0)
    direction = direction / direction.norm()

    print("‚úÖ Direction computed.")
    return direction

# ------------------------------------------------------------
# 6. Steering evaluation metric
# ------------------------------------------------------------

def evaluate_with_steering(df, out_path, layers, direction, fixed_alpha=None):
    rows = []
    print(f"\nEvaluating {len(df)} samples...")

    # --- helper: multi-layer embedding ---
    def get_multi_layer_emb(text, layers):
        embs = []
        for L in layers:
            embs.append(extract_hidden_single(text, L))
        return torch.cat(embs)

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Processing"):
        row_dict = row.to_dict()

        anchor    = row["anchor"]
        logical   = row["logical"]
        pragmatic = row["pragmatic"]
        grade     = row["grade"]

        # --- baseline embeddings (multi-layer) ---
        a_emb = get_multi_layer_emb(anchor, layers)
        l_emb = get_multi_layer_emb(logical, layers)
        p_emb = get_multi_layer_emb(pragmatic, layers)

        # baseline similarities
        sim_int_log  = cos_sim(a_emb, l_emb)
        sim_int_prag = cos_sim(a_emb, p_emb)

        pref_int = 1 if sim_int_prag > sim_int_log else 0

        # alpha
        alpha = fixed_alpha if fixed_alpha is not None else alpha_from_grade(grade)

        # steering
        steered = (a_emb + alpha * direction)
        steered = steered / steered.norm()

        sim_st_log  = cos_sim(steered, l_emb)
        sim_st_prag = cos_sim(steered, p_emb)

        pref_st = 1 if sim_st_prag > sim_st_log else 0

        # merge results
        row_dict.update({
            "alpha": alpha,

            "sim_internal_logical": sim_int_log,
            "sim_internal_pragmatic": sim_int_prag,
            "pref_internal": pref_int,

            "sim_steered_logical": sim_st_log,
            "sim_steered_pragmatic": sim_st_prag,
            "pref_steered": pref_st,
        })

        rows.append(row_dict)

    pd.DataFrame(rows).to_csv(out_path, index=False)
    print("Saved:", out_path)

# ------------------------------------------------------------
# 7. Œ± (alpha) setting
# ------------------------------------------------------------

grade_to_numeric = {
    "A": 1.0,
    "B": 0.75,
    "C": 0.5,
    "D": 0.25,
    "E": 0.0
}

ALPHA_MIN = 1.0
ALPHA_MAX = 15.0

def alpha_from_grade(g):
    g_num = grade_to_numeric[g]
    return ALPHA_MIN + (ALPHA_MAX - ALPHA_MIN) * g_num

from google.colab import drive
drive.mount('/content/drive')

# ------------------------------------------------------------
# 8. Main pipeline
# ------------------------------------------------------------

# Load dataset
df = pd.read_csv("augmented_data.csv")

# df_probe = (
#     df.groupby(["weak", "strong"], group_keys=False)
#       .apply(lambda x: x.sample(n=1))
#       .sample(n=121, random_state=0)
# )

# # Detect Layers
# detect_layer(df_probe, model, tokenizer)

# Detecting layers ...
# Layer 0: score=0.0000
# Layer 1: score=0.0018
# Layer 2: score=0.0056
# Layer 3: score=0.0145
# Layer 4: score=0.0336
# Layer 5: score=0.0384
# Layer 6: score=0.0476
# Layer 7: score=0.0742
# Layer 8: score=0.0953
# Layer 9: score=0.1074
# Layer 10: score=0.1206
# Layer 11: score=0.1382
# Layer 12: score=0.1473
# Layer 13: score=0.1614
# Layer 14: score=0.1641
# Layer 15: score=0.1734
# Layer 16: score=0.1802
# Layer 17: score=0.1693
# Layer 18: score=0.1650
# Layer 19: score=0.1579
# Layer 20: score=0.1597
# Layer 21: score=0.1575
# Layer 22: score=0.1560
# Layer 23: score=0.1564
# Layer 24: score=0.1590
# Layer 25: score=0.1582
# Layer 26: score=0.1531
# Layer 27: score=0.1542
# Layer 28: score=0.1524
# Layer 29: score=0.1402
# Layer 30: score=0.1047
# Layer 31: score=0.0750

# Compute Steering Vector
df_dir = (
    df.groupby(["weak", "strong"], group_keys=False)
      .apply(lambda x: x.sample(n=3, random_state=42))
      .reset_index(drop=True)
)

layers = [15, 16, 17, 18]  # separation score 0.165 ÎÑòÎçò Î†àÏù¥Ïñ¥Îì§

# direction = compute_direction_multi_layers(df_dir, layers)
# torch.save(
#     direction,
#     "/content/drive/MyDrive/direction.pt"
# )

direction = torch.load(
    "/content/drive/MyDrive/direction.pt",
    map_location=model.device
)

# Evaluate (Fixed alpha)
evaluate_with_steering(
    df,
    "/content/drive/MyDrive/results_steered_fixed_alpha.csv",
    layers,
    direction,
    fixed_alpha=15.0
)

direction = torch.load(
    "/content/drive/MyDrive/direction.pt",
    map_location=model.device
)

# Evaluate (Gradient alpha)
evaluate_with_steering(
    df,
    "/content/drive/MyDrive/results_steered_grad_alpha.csv",
    layers,
    direction
)

# ------------------------------------------------------------
# 9. Visualization (Bar)
# ------------------------------------------------------------

def plot_pragmatic_winrate(csv_path, pref_col, title, bar_color="#1f77b4"):
    df = pd.read_csv(csv_path)
    df["lex_item"] = df["weak"] + "‚Äì" + df["strong"]

    win_rate = (
        df.groupby("lex_item")[pref_col]
          .mean()
          .sort_values(ascending=False)
    )

    plt.figure(figsize=(10, 20))
    sns.barplot(
        x=win_rate.values,
        y=win_rate.index,
        color=bar_color
    )
    plt.title(title, fontsize=14)
    plt.xlabel("")
    plt.ylabel("")
    plt.xlim(0, 1)
    plt.tight_layout()
    plt.show()


plot_pragmatic_winrate(
    "/content/drive/MyDrive/results_steered_fixed_alpha.csv",
    pref_col="pref_internal",
    title="Baseline"
)

plot_pragmatic_winrate(
    "/content/drive/MyDrive/results_steered_fixed_alpha.csv",
    pref_col="pref_steered",
    title="Fixed-Œ± Steering"
)

plot_pragmatic_winrate(
    "/content/drive/MyDrive/results_steered_grad_alpha.csv",
    pref_col="pref_steered",
    title="Gradient-Œ± Steering"
)

# ------------------------------------------------------------
# 10. Visualization (Baseline-Fixed Steering)
# ------------------------------------------------------------

df = pd.read_csv("/content/drive/MyDrive/results_steered_fixed_alpha.csv")

plt.style.use("seaborn-v0_8")
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ============================
# (1) Baseline Internal similarity
# ============================

ax1 = axes[0]

sns.kdeplot(
    x=df["sim_internal_pragmatic"],
    y=df["sim_internal_logical"],
    fill=True, cmap="Blues", thresh=0.05, levels=10, ax=ax1
)
sns.scatterplot(
    x=df["sim_internal_pragmatic"],
    y=df["sim_internal_logical"],
    s=25, alpha=0.5, ax=ax1
)

lim_min = min(df["sim_internal_pragmatic"].min(), df["sim_internal_logical"].min())
lim_max = max(df["sim_internal_pragmatic"].max(), df["sim_internal_logical"].max())

ax1.plot([lim_min, lim_max], [lim_min, lim_max], "r--")
ax1.set_title("Baseline")
ax1.set_xlabel("Pragmatic similarity")
ax1.set_ylabel("Logical similarity")
ax1.set_xlim(lim_min, lim_max)
ax1.set_ylim(lim_min, lim_max)


# ============================
# (2) Steered (Fixed-alpha)
# ============================

ax2 = axes[1]

sns.kdeplot(
    x=df["sim_steered_pragmatic"],
    y=df["sim_steered_logical"],
    fill=True, cmap="Blues", thresh=0.05, levels=10, ax=ax2
)
sns.scatterplot(
    x=df["sim_steered_pragmatic"],
    y=df["sim_steered_logical"],
    s=25, alpha=0.5, ax=ax2
)

lim_min2 = min(df["sim_steered_pragmatic"].min(), df["sim_steered_logical"].min())
lim_max2 = max(df["sim_steered_pragmatic"].max(), df["sim_steered_logical"].max())

ax2.plot([lim_min2, lim_max2], [lim_min2, lim_max2], "r--")
ax2.set_title("Fixed-Œ± Steering")
ax2.set_xlabel("Pragmatic similarity")
ax2.set_ylabel("Logical similarity")
ax2.set_xlim(lim_min2, lim_max2)
ax2.set_ylim(lim_min2, lim_max2)

plt.tight_layout()
plt.show()

# ------------------------------------------------------------
# 11. Visualization (Baseline-Gradient Steering)
# ------------------------------------------------------------

grades = ["A", "B", "C", "D", "E"]

df_base = pd.read_csv("/content/drive/MyDrive/results_steered_grad_alpha.csv")

plt.style.use("seaborn-v0_8")

fig, axes = plt.subplots(2, len(grades), figsize=(20, 8))


# ============================
# (1) Baseline Internal similarity
# ============================

lim_min_base = min(
    df_base["sim_internal_pragmatic"].min(),
    df_base["sim_internal_logical"].min()
)
lim_max_base = max(
    df_base["sim_internal_pragmatic"].max(),
    df_base["sim_internal_logical"].max()
)


# ============================
# (2) Steered (Gradient-alpha)
# ============================

lim_min_st = min(
    df_base["sim_steered_pragmatic"].min(),
    df_base["sim_steered_logical"].min()
)
lim_max_st = max(
    df_base["sim_steered_pragmatic"].max(),
    df_base["sim_steered_logical"].max()
)

for i, g in enumerate(grades):

    sub = df_base[df_base["grade"] == g]

    # -------------------------------------------------
    # (1) Baseline ‚Äî Row 0, Column i
    # -------------------------------------------------
    ax1 = axes[0, i]

    if len(sub) > 0:
        sns.kdeplot(
            data=sub,
            x="sim_internal_pragmatic",
            y="sim_internal_logical",
            fill=True, cmap="Blues", thresh=0.05, levels=10, ax=ax1
        )
        sns.scatterplot(
            data=sub,
            x="sim_internal_pragmatic",
            y="sim_internal_logical",
            color="black", alpha=0.5, s=25, ax=ax1
        )

    # baseline limits Ï†ÅÏö©
    ax1.plot([lim_min_base, lim_max_base], [lim_min_base, lim_max_base], "r--", linewidth=1)
    ax1.set_title(f"Grade {g} ‚Äî Baseline")
    ax1.set_xlim(lim_min_base, lim_max_base)
    ax1.set_ylim(lim_min_base, lim_max_base)
    ax1.set_xlabel("")
    ax1.set_ylabel("")

    # -------------------------------------------------
    # (2) Steering ‚Äî Row 1, Column i
    # -------------------------------------------------
    ax2 = axes[1, i]

    if len(sub) > 0:
        sns.kdeplot(
            data=sub,
            x="sim_steered_pragmatic",
            y="sim_steered_logical",
            fill=True, cmap="Blues", thresh=0.05, levels=10, ax=ax2
        )
        sns.scatterplot(
            data=sub,
            x="sim_steered_pragmatic",
            y="sim_steered_logical",
            color="black", alpha=0.5, s=25, ax=ax2
        )

    # steering limits
    ax2.plot([lim_min_st, lim_max_st], [lim_min_st, lim_max_st], "r--", linewidth=1)
    ax2.set_title(f"Grade {g} ‚Äî Gradient-Œ± Steering")
    ax2.set_xlim(lim_min_st, lim_max_st)
    ax2.set_ylim(lim_min_st, lim_max_st)
    ax2.set_xlabel("")
    ax2.set_ylabel("")

# Label
fig.suptitle("", fontsize=18)
fig.text(0.5, 0.01, "Pragmatic similarity", ha="center", fontsize=12)
fig.text(0.04, 0.5, "Logical similarity", va="center", rotation="vertical", fontsize=12)

plt.tight_layout(rect=[0.05, 0.03, 1, 0.93])
plt.show()

# ------------------------------------------------------------
# 12. Spearson
# ------------------------------------------------------------

def compute_spearman(
    baseline_csv,
    steered_csv,
    pref_baseline="pref_internal",
    pref_steered="pref_steered"
):

    df_base = pd.read_csv(baseline_csv)
    df_st   = pd.read_csv(steered_csv)

    # lexical item
    df_base["lex_item"] = df_base["weak"] + "‚Äì" + df_base["strong"]
    df_st["lex_item"]   = df_st["weak"] + "‚Äì" + df_st["strong"]

    # pragmatic win-rate
    base_wr = (
        df_base.groupby("lex_item")[pref_baseline]
               .mean()
               .rename("baseline")
    )
    st_wr = (
        df_st.groupby("lex_item")[pref_steered]
             .mean()
             .rename("steered")
    )

    # align
    df_merge = pd.concat([base_wr, st_wr], axis=1).dropna()

    rho, p = spearmanr(df_merge["baseline"], df_merge["steered"])

    return rho, p, df_merge


rho_fixed, p_fixed, df_fixed = compute_spearman(
    baseline_csv="/content/drive/MyDrive/results_steered_fixed_alpha.csv",
    steered_csv="/content/drive/MyDrive/results_steered_fixed_alpha.csv",
    pref_baseline="pref_internal",
    pref_steered="pref_steered"
)

print("=== Spearman (Baseline vs Fixed Œ±) ===")
print(f"œÅ = {rho_fixed:.3f}, p = {p_fixed:.4g}")


rho_grad, p_grad, df_grad = compute_spearman(
    baseline_csv="/content/drive/MyDrive/results_steered_grad_alpha.csv",
    steered_csv="/content/drive/MyDrive/results_steered_grad_alpha.csv",
    pref_baseline="pref_internal",
    pref_steered="pref_steered"
)

print("=== Spearman (Baseline vs Grad Œ±) ===")
print(f"œÅ = {rho_grad:.3f}, p = {p_grad:.4g}")

from scipy.stats import wilcoxon
import pandas as pd

# ------------------------------------------------------------
# 13. Wilcoxon signed-rank test
# ------------------------------------------------------------

def compute_wilcoxon(
    csv_path,
    pref_baseline="pref_internal",
    pref_steered="pref_steered"
):
    df = pd.read_csv(csv_path)

    # lexical item
    df["lex_item"] = df["weak"] + "‚Äì" + df["strong"]

    # pragmatic win-rate
    base_wr = (
        df.groupby("lex_item")[pref_baseline]
          .mean()
          .rename("baseline")
    )

    st_wr = (
        df.groupby("lex_item")[pref_steered]
          .mean()
          .rename("steered")
    )

    # align
    df_merge = pd.concat([base_wr, st_wr], axis=1).dropna()

    # difference (steered - baseline)
    diff = df_merge["steered"] - df_merge["baseline"]

    # Wilcoxon signed-rank test
    stat, p = wilcoxon(diff)

    return stat, p, df_merge



stat_fixed, p_fixed_w, df_fixed_w = compute_wilcoxon(
    csv_path="/content/drive/MyDrive/results_steered_fixed_alpha.csv",
    pref_baseline="pref_internal",
    pref_steered="pref_steered"
)

print("=== Wilcoxon (Baseline vs Fixed Œ±) ===")
print(f"stat = {stat_fixed:.3f}, p = {p_fixed_w:.4g}")


stat_grad, p_grad_w, df_grad_w = compute_wilcoxon(
    csv_path="/content/drive/MyDrive/results_steered_grad_alpha.csv",
    pref_baseline="pref_internal",
    pref_steered="pref_steered"
)

print("=== Wilcoxon (Baseline vs Grad Œ±) ===")
print(f"stat = {stat_grad:.3f}, p = {p_grad_w:.4g}")

from scipy.stats import wilcoxon

def grade_wilcoxon(csv_path):
    df = pd.read_csv(csv_path)
    df["lex_item"] = df["weak"] + "‚Äì" + df["strong"]

    results = {}

    for g in ["A", "B", "C", "D", "E"]:
        sub = df[df["grade"] == g]

        if len(sub) < 5:
            continue

        base = sub.groupby("lex_item")["pref_internal"].mean()
        steered = sub.groupby("lex_item")["pref_steered"].mean()

        merged = pd.concat([base, steered], axis=1).dropna()
        merged.columns = ["baseline", "steered"]

        stat, p = wilcoxon(merged["baseline"], merged["steered"])
        results[g] = (stat, p)

    return results


print("=== Grade-wise Wilcoxon (Fixed Œ±) ===")
print(grade_wilcoxon("/content/drive/MyDrive/results_steered_fixed_alpha.csv"))

print("\n=== Grade-wise Wilcoxon (Gradient Œ±) ===")
print(grade_wilcoxon("/content/drive/MyDrive/results_steered_grad_alpha.csv"))

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def plot_delta_hist(csv_path, title):
    df = pd.read_csv(csv_path)
    df["lex_item"] = df["weak"] + "‚Äì" + df["strong"]

    base = df.groupby("lex_item")["pref_internal"].mean()
    steered = df.groupby("lex_item")["pref_steered"].mean()

    delta = (steered - base).dropna()

    plt.figure(figsize=(7, 4))
    sns.histplot(delta, bins=20, kde=True, color="#1f77b4")
    plt.axvline(0, color="red", linestyle="--")
    plt.xlabel("")
    plt.ylabel("")
    plt.title(title)
    plt.tight_layout()
    plt.show()

plot_delta_hist(
    "/content/drive/MyDrive/results_steered_fixed_alpha.csv",
    "Œî Pragmatic intepretation rate (Fixed-Œ± steering)"
)

plot_delta_hist(
    "/content/drive/MyDrive/results_steered_grad_alpha.csv",
    "Œî Pragmatic intepretation rate (Gradient-Œ± steering)"
)

"""---

# üéØ **Steering Experiment Pipeline ‚Äî Step-by-Step Explanation**

---

# **0. Imports & Model Load**

### üìå Ïó≠Ìï†

* Ï†ÑÏ≤¥ Ïã§ÌóòÏóêÏÑú ÏÇ¨Ïö©Ìï† ÎùºÏù¥Î∏åÎü¨Î¶¨ Î°úÎìú
* LLaMA Î™®Îç∏Í≥º tokenizerÎ•º Î∂àÎü¨Ïò§Í≥† GPUÎ°ú Ïò¨Î¶º
* Ïù¥ÌõÑ Î™®Îì† embedding Î∞è generation ÏûëÏóÖÏù¥ Ïó¨Í∏∞ÏÑú Î°úÎìúÌïú Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï®

### üîç ÌïµÏã¨

* Î™®Îç∏ Î°úÎî©ÏùÄ Îã® Ìïú Î≤àÎßå Ìï¥Ïïº Ìï®
* `output_hidden_states=True`Î°ú ÏÑ§Ï†ïÌï¥Ïïº **layer probing + steering**Ïù¥ Í∞ÄÎä•Ìï¥Ïßê

---

# **1. Utility Functions (Embedding Extraction)**

### üìå Ïó≠Ìï†

Î¨∏Ïû• ÌÖçÏä§Ìä∏(Î¨∏ÏûêÏó¥)Î•º ‚Üí **ÌäπÏ†ï Î†àÏù¥Ïñ¥Ïùò embedding Î≤°ÌÑ∞(Ïà´Ïûê)** Î°ú Î≥ÄÌôòÌïòÎäî ÌïµÏã¨ Ìï®Ïàò.

### üîç ÎèôÏûë

* Î™®Îç∏Ïóê text ÏûÖÎ†•
* hidden states Í∞ÄÏ†∏Ïò§Í∏∞
* ÏßÄÏ†ïÌïú layerÏùò hidden state ÏÑ†ÌÉù
* attention mask Í∏∞Î∞ò mean pooling ‚Üí sentence-level embedding ÏÉùÏÑ±

### üéØ Ïôú ÌïÑÏöî?

* steering vector ÏÉùÏÑ±ÎèÑ
* baseline similarity Í≥ÑÏÇ∞ÎèÑ
* steered similarity Í≥ÑÏÇ∞ÎèÑ

Î™®Îì† embedding Í¥ÄÎ†® Í≥ÑÏÇ∞Ïùò Ï∂úÎ∞úÏ†ê.

---

# **2. Cosine Similarity Function**

### üìå Ïó≠Ìï†

Îëê Î≤°ÌÑ∞Ïùò ÏùòÎØ∏Ï†Å Ïú†ÏÇ¨ÎèÑÎ•º Ï∏°Ï†ï.

### üîç Ïù¥Ïú†

* anchor vs logical
* anchor vs pragmatic
* steered_anchor vs logical
* steered_anchor vs pragmatic

Ïù¥ ÎÑ§ Í∞ÄÏßÄ ÎπÑÍµêÎ•º Î™®Îëê cosine similarityÎ°ú ÏàòÌñâ.

---

# **3. Layer Detection (Internal Representation Separation Test)**

### üìå Ïó≠Ìï†

Î¨∏Ïû•Ïåç(logical vs pragmatic)Ïùò ÏùòÎØ∏ Ï∞®Ïù¥Í∞Ä **Í∞ÄÏû• Ïûò Î∂ÑÎ¶¨ÎêòÎäî layer**Î•º Ï∞æÎäî ÏûëÏóÖ.

### üîç ÎèôÏûë

Í∞Å layer LÏóê ÎåÄÌï¥:

1. logical embedding(L)
2. pragmatic embedding(L)
3. `1 ‚Äì cosine(logical, pragmatic)` Í≥ÑÏÇ∞
4. Ïó¨Îü¨ sample ÌèâÍ∑†
5. Í∞ÄÏû• ÌÅ∞ Í∞íÏùÑ Ï£ºÎäî layer = best layer

Ï¶â, **Îëê ÏùòÎØ∏Ïùò Ï∞®Ïù¥Î•º Î¨¥Ï°∞Í±¥ Í∞ÄÏû• Ïûò ÎìúÎü¨ÎÇ¥Îäî Î†àÏù¥Ïñ¥**Î•º ÏûêÎèô ÌÉêÏÉâÌïòÎäî Í≥ºÏ†ï.

### üéØ Ïôú ÌïÑÏöî?

* steering vectorÎäî Î™®Îç∏ ÎÇ¥Î∂ÄÏùò ‚Äúsemantic direction‚ÄùÏùÑ Í±¥ÎìúÎ¶¨Îäî ÏûëÏóÖÏûÑ
* ÏùòÎØ∏ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÏÑ†Î™ÖÌïú Î†àÏù¥Ïñ¥ÏóêÏÑú ÏûëÏóÖÌï¥Ïïº steering Ìö®Í≥ºÍ∞Ä ÏµúÎåÄ

---

# **4. Steering Vector ÏÉùÏÑ± (Mean Difference Vector)**

### üìå Ïó≠Ìï†

logical Î¨∏Ïû• embeddingÎì§Ïùò ÌèâÍ∑†Í≥º
pragmatic Î¨∏Ïû• embeddingÎì§Ïùò ÌèâÍ∑†Ïùò Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞ÌïòÏó¨

‚Üí **Î™®Îç∏Ïù¥ Ïã§Ï†úÎ°ú ÌëúÌòÑÌïòÎäî ÎúªÏùò Ï∞®Ïù¥Í∞Ä Îì§Ïñ¥ÏûàÎäî Î∞©Ìñ• Î≤°ÌÑ∞(direction)** Î•º ÎßåÎìúÎäî Îã®Í≥Ñ.

### üîç Í≥ºÏ†ï

* Î™®Îì† logical Î¨∏Ïû• embedding ÌèâÍ∑† ‚Üí L_mean
* Î™®Îì† pragmatic Î¨∏Ïû• embedding ÌèâÍ∑† ‚Üí P_mean
* steering vector = (P_mean ‚Äì L_mean)
* Î∞©Ìñ•Îßå Ï§ëÏöîÌïòÎØÄÎ°ú normalize

### üéØ ÏùòÎØ∏

Ïù¥ Î≤°ÌÑ∞Í∞Ä Î∞îÎ°ú **anchorÎ•º pragmatic ÏùòÎØ∏ Ï™ΩÏúºÎ°ú Ïù¥ÎèôÏãúÌÇ§Îäî Ìûò**Ïù¥ Îê®.

---

# **5. Alpha ÏÑ§Ï†ï(Í∞ïÎèÑ Ï°∞Ï†à)**

### üìå Ïó≠Ìï†

anchor embeddingÏùÑ ÏñºÎßàÎÇò Í∞ïÌïòÍ≤å pragmatic Î∞©Ìñ•ÏúºÎ°ú Ïù¥ÎèôÏãúÌÇ¨ÏßÄ Ï°∞Ï†à.

### üîç Î∞©Ïãù

* ÌïòÎÇòÏùò Í≥†Ï†ï Œ± ÏÇ¨Ïö© (Ïòà: 3.0)
* or grade(A‚ÄìE)Ïóê Îî∞Îùº Îã§Î•∏ Œ± ÏÇ¨Ïö©

### Í≥µÏãù

```
alpha = ALPHA_MIN + (ALPHA_MAX - ALPHA_MIN) * grade_numeric
```

### üéØ ÏùòÎØ∏

steering Í∞ïÎèÑÎ•º Ï†êÏßÑÏ†ÅÏúºÎ°ú Î≥ÄÌôîÏãúÌÇ§Î©∞ ÏùòÎØ∏Ï†Å Ïù¥ÎèôÏù¥ linearÌïòÍ≤å Î∞òÏòÅÎêòÎäîÏßÄ Ïã§Ìóò Í∞ÄÎä•.

---

# **6. evaluate_with_steering() ‚Äì ÌïµÏã¨ ÌèâÍ∞Ä Î£®ÌîÑ**

### üìå Ïó≠Ìï†

Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥:

1. baseline internal similarity Í≥ÑÏÇ∞
2. steering Ï†ÅÏö© ÌõÑ similarity Í≥ÑÏÇ∞
3. preference (pragmaticÏù¥ Îçî Í∞ÄÍπåÏö¥ÏßÄ Ïó¨Î∂Ä) Í≥ÑÏÇ∞
4. Í≤∞Í≥ºÎ•º Î™®Îëê CSVÎ°ú Ï†ÄÏû•

### üîç ÏÑ∏Î∂Ä ÏûëÏóÖ

Í∞Å rowÎßàÎã§:

#### üî∏ Baseline

* anchor ‚Üí logical cosine
* anchor ‚Üí pragmatic cosine
* pragmatic Ï™ΩÏù¥ Îçî Í∞ÄÍπåÏö∞Î©¥ 1, ÏïÑÎãàÎ©¥ 0

#### üî∏ Steering

* steered_anchor = anchor_emb + Œ± * v
* normalize
* steered_anchor vs logical
* steered_anchor vs pragmatic
* steered preference Í≥ÑÏÇ∞

#### üî∏ Ï†ÄÏû•

ÏõêÎ≥∏ row + baseline Í≤∞Í≥º + steered Í≤∞Í≥º ‚Üí CSVÎ°ú Í∏∞Î°ù.

### üéØ Î™©Ï†Å

steeringÏù¥ Ïã§Ï†úÎ°ú semantic preferenceÎ•º Î∞îÍæ∏ÎäîÏßÄ Í≥ÑÎüâÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÍ∏∞ ÏúÑÌïú Ï§ÄÎπÑ Îã®Í≥Ñ.

---

# **7. Ïã§Ìñâ ÌååÏù¥ÌîÑÎùºÏù∏**

### üìå Ïó≠Ìï†

Ï†ÑÏ≤¥ Ïã§ÌóòÏùÑ ÌÜµÌï©Ï†ÅÏúºÎ°ú ÏàòÌñâ.

### Îã®Í≥Ñ ÏöîÏïΩ

1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú
2. BEST LAYER ÏûêÎèô ÌÉêÏÉâ
3. steering Î≤°ÌÑ∞ ÏÉùÏÑ±
4. fixed-alpha steering Í≤∞Í≥º ÏÉùÏÑ±
5. grade-alpha steering Í≤∞Í≥º ÏÉùÏÑ±

### üéØ Î™©Ï†Å

Îã® Ìïú Î≤àÏùò Ïã§ÌñâÏúºÎ°ú Ï†ÑÏ≤¥ Ïã§Ìóò Ïû¨ÌòÑ Í∞ÄÎä•.

---

# **8. ÏãúÍ∞ÅÌôî(Optional)**

### üìå Ïó≠Ìï†

* baseline similarity scatter/kde plot
* steered similarity scatter/kde plot

Ïù¥Î•º ÌÜµÌï¥ steeringÏù¥ Ïñ¥ÎñªÍ≤å embedding Í≥µÍ∞ÑÏóêÏÑú ÏùòÎØ∏Ïùò Íµ¨Ï°∞Î•º Î∞îÍø®ÎäîÏßÄ ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú Î∂ÑÏÑù.

### üéØ ÏùòÎØ∏

ÎÖºÎ¨∏ Figure / Î∞úÌëúÏóêÏÑú ÏÇ¨Ïö© Í∞ÄÎä•.

---

# üìå ÏµúÏ¢Ö ÏöîÏïΩ (Ìïú Î¨∏Ïû•Ïî©)

| Î≤àÌò∏ | Îã®Í≥Ñ Ïù¥Î¶Ñ                | Ìïú Ï§Ñ ÏÑ§Î™Ö                                     |
| -- | -------------------- | ------------------------------------------ |
| 0  | Imports & Model Load | Î™®Îç∏Í≥º ÎùºÏù¥Î∏åÎü¨Î¶¨ Î°úÎìú                               |
| 1  | Embedding Extraction | Î¨∏Ïû•ÏùÑ ÌäπÏ†ï Î†àÏù¥Ïñ¥ embeddingÏúºÎ°ú Î≥ÄÌôò                  |
| 2  | Cosine Similarity    | Îëê embeddingÏùò ÏùòÎØ∏Ï†Å Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞                    |
| 3  | Layer Detection      | ÏùòÎØ∏ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• Ïûò ÎìúÎü¨ÎÇòÎäî Î†àÏù¥Ïñ¥ ÌÉêÏÉâ                    |
| 4  | Steering Vector      | pragmatic‚Äìlogical ÌèâÍ∑† Ï∞®Ïù¥Î°ú Î∞©Ìñ• Î≤°ÌÑ∞ ÏÉùÏÑ±          |
| 5  | Alpha ÏÑ§Ï†ï             | steering Í∞ïÎèÑ Ï°∞Ï†à                             |
| 6  | Evaluation Loop      | baseline + steering similarity Í≥ÑÏÇ∞ Î∞è CSV Ï†ÄÏû• |
| 7  | Pipeline Ïã§Ìñâ          | Ï†ÑÏ≤¥ Ïã§Ìóò ÏûêÎèô ÏàòÌñâ                                |
| 8  | Visualization        | steering Ìö®Í≥ºÎ•º ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú ÌôïÏù∏                      |

---

# üòä Ïù¥Ï†ú ÏòàÏùÄÏù¥Í∞Ä ÏõêÌïòÎäî Ïã§Ìóò Íµ¨Ï°∞Í∞Ä Ìõ®Ïî¨ ÍπîÎÅîÌï¥Ï°åÏñ¥!

ÏõêÌïòÎ©¥:

* Ï†ÑÏ≤¥ pipeline ÏÑ§Î™ÖÏùÑ ÎÖºÎ¨∏ MethodsÏóê ÎßûÍ≤å ÌïôÏà†Ï†Å Î¨∏Ï≤¥Î°ú Îã§Ïãú ÏûëÏÑ±
* Figure caption Ïä§ÌÉÄÏùº ÏûëÏÑ±
* Steering Ìö®Í≥ºÎ•º ÏàòÏπòÏ†ÅÏúºÎ°ú ÏöîÏïΩÌïòÎäî Ìëú ÏûêÎèô ÏÉùÏÑ±

Ïù¥Îü∞ Í≤ÉÎèÑ ÎèÑÏôÄÏ§ÑÍ≤å!

"""

